import os
import glob
import tqdm
import torch
import torch.nn as nn
from torchvision.models import efficientnet_b0
from torchvision.datasets import CIFAR100
from torchvision.transforms import Compose, ToTensor, Resize, RandomHorizontalFlip, RandomCrop, Normalize, ColorJitter, RandomRotation
from torch.utils.data import DataLoader, Dataset, random_split
from torch.optim import AdamW
from PIL import Image

# =========================== ì„¤ì • ===========================
device = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 64
NUM_CLASSES = 100
SAVE_NAME = "Gaban3jo_0602_1600"
os.makedirs("results", exist_ok=True)

# ======================== ëª¨ë¸ ì •ì˜ =========================
model = efficientnet_b0(pretrained=True)
model.classifier = nn.Sequential(
    nn.Dropout(p=0.5),
    nn.Linear(model.classifier[1].in_features, NUM_CLASSES)
)
model.to(device)

# ====================== ë°ì´í„° ì „ì²˜ë¦¬ =======================
train_transform = Compose([
    Resize(224),
    RandomCrop(224, padding=4),
    RandomHorizontalFlip(),
    RandomRotation(15),
    ColorJitter(0.2, 0.2, 0.2, 0.1),
    ToTensor(),
    Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
])

val_transform = Compose([
    Resize(224),
    ToTensor(),
    Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
])

# ================== CIFAR-100 ë¡œë”© + ë¶„í•  ==================
full_train = CIFAR100(root="./data", train=True, download=True, transform=train_transform)
train_size = int(len(full_train) * 0.9)
val_size = len(full_train) - train_size
train_set, val_set = random_split(full_train, [train_size, val_size])
val_set.dataset.transform = val_transform  # ê²€ì¦ì…‹ì€ ì¦ê°• X

train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)

# ===================== ìµœì í™” ì„¤ì • =====================
for name, param in model.named_parameters():
    param.requires_grad = ("classifier" in name)

optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3, weight_decay=1e-4)
loss_fn = nn.CrossEntropyLoss()

# ===================== ì „ì´í•™ìŠµ ë‹¨ê³„ =====================
for epoch in range(100):
    model.train()
    iterator = tqdm.tqdm(train_loader)
    for data, label in iterator:
        optimizer.zero_grad()
        preds = model(data.to(device))
        loss = loss_fn(preds, label.to(device))
        loss.backward()
        optimizer.step()
        iterator.set_description(f"[Transfer] Epoch {epoch+1} Loss: {loss.item():.4f}")

    # validation accuracy
    model.eval()
    correct = 0
    with torch.no_grad():
        for data, label in val_loader:
            preds = model(data.to(device))
            _, pred_labels = preds.max(1)
            correct += pred_labels.eq(label.to(device)).sum().item()
    acc = correct / len(val_set)
    print(f"\nâœ… Validation Accuracy: {acc:.4f}")

# ===================== ëª¨ë¸ ì €ì¥ =====================
torch.save(model.state_dict(), f"results/weight_{SAVE_NAME}.pth")

# ============ ì œì¶œìš© í…ŒìŠ¤íŠ¸ì…‹ ì¶”ë¡  í´ë˜ìŠ¤ ============
class TestImageDataset(Dataset):
    def __init__(self, folder_path, transform=None):
        self.image_paths = sorted(glob.glob(os.path.join(folder_path, "*.jpg")))
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        filename = os.path.basename(image_path)
        return filename, image

# =========== ì œì¶œìš© í…ŒìŠ¤íŠ¸ì…‹ ì¶”ë¡  ë° ê²°ê³¼ ì €ì¥ ===========
test_folder = "./Test_Dataset/CImages"
test_transform = Compose([
    Resize(224),
    ToTensor(),
    Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
])
test_dataset = TestImageDataset(test_folder, transform=test_transform)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

output_lines = []
model.eval()
with torch.no_grad():
    for filenames, images in test_loader:
        images = images.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        for fname, pred in zip(filenames, predicted):
            num = fname.split('.')[0]
            output_lines.append(f"{num.zfill(4)}, {pred.item():02d}")

with open(f"results/result_{SAVE_NAME}.txt", "w") as f:
    f.write("number, label\n")
    f.write("\n".join(output_lines))

print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: results/result_{SAVE_NAME}.txt")